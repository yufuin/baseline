[project]
name = "deepspeed_pipeline_parallel"
version = "0.1.0"
description = "Add your description here"
readme = "README.md"
requires-python = "==3.12.8"
dependencies = [
    "accelerate>=1.9.0",
    "deepspeed>=0.17.3",
    "flash-attn>=2.8.1",
    "ipykernel>=6.30.0",
    "numpy>=2.3.2",
    "pandas>=2.3.1",
    "pydantic>=2.11.7",
    "seaborn>=0.13.2",
    "torch==2.7.1",
    "transformers>=4.54.0",
    "wandb>=0.21.0",
]

[[tool.uv.index]]
name = "torch-cuda"
url = "https://download.pytorch.org/whl/cu128"
explicit = true

[tool.uv.sources]
torch = [{ index = "torch-cuda" }]
flash-attn = { url = "https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.13/flash_attn-2.8.1+cu128torch2.7-cp312-cp312-linux_x86_64.whl" }
